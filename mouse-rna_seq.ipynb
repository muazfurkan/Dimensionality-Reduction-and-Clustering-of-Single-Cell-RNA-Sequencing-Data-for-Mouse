{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "gkHg57EJpSeB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.decomposition import PCA, TruncatedSVD, KernelPCA\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.cluster import KMeans\n",
        "from mpl_toolkits import mplot3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "edjPEMlpyRYA"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/mouse/nestorawa_forcellcycle_expressionMatrix.txt\", delimiter=\"\\t\")\n",
        "df2 = pd.read_csv(\"/content/drive/MyDrive/mouse/nestorowa_corrected_population_annotation.txt\")\n",
        "class_count = len(np.unique(df2['celltype']))\n",
        "\n",
        "rows = len(df)\n",
        "df = df.sample(n=int(rows/3))\n",
        "\n",
        "scaled_df = pd.DataFrame()\n",
        "reel_classes = []\n",
        "\n",
        "class_dict = {}\n",
        "mapped = []\n",
        "count = 0\n",
        "\n",
        "for i in df2['celltype']:\n",
        "  class_label = i.split()[1]\n",
        "  if class_label not in class_dict:\n",
        "    class_dict[class_label] = count\n",
        "    count += 1\n",
        "\n",
        "  mapped.append(class_dict[class_label])\n",
        "  reel_classes.append(class_label)\n",
        "\n",
        "class_count = len(np.unique(reel_classes))\n",
        "print(class_count)\n",
        "print(np.unique(reel_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "LNUrljobqQ0w"
      },
      "outputs": [],
      "source": [
        "def scaling(column):\n",
        "  values = []\n",
        "  scaled_val = []\n",
        "  for i in df.iloc[:, column]:\n",
        "    values.append(i)\n",
        "  mean_val = np.mean(values)\n",
        "  std = np.std(values)\n",
        "  for i in df.iloc[:, column]:\n",
        "    i = (i-mean_val) / std\n",
        "    scaled_val.append(i)\n",
        "  scaled_df[df.columns[column]] = scaled_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "dxLJQqKDwpZ_"
      },
      "outputs": [],
      "source": [
        "for i in range(df.shape[1]):\n",
        "  scaling(i)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MY TOOLS\n",
        "\n",
        "class reduction:\n",
        "  \n",
        "  def __init__(self,\n",
        "               train_data,\n",
        "               dimension):\n",
        "    \n",
        "    self.train_data = train_data\n",
        "    self.dimension = dimension\n",
        "\n",
        "  def pca(self):\n",
        "    from sklearn.decomposition import PCA\n",
        "\n",
        "    pca = PCA(n_components=self.dimension)\n",
        "    reducted_data = pca.fit_transform(self.train_data)\n",
        "\n",
        "    return reducted_data\n",
        "\n",
        "  def kernel_pca(self,\n",
        "                 kernel):\n",
        "    \n",
        "    from sklearn.decomposition import KernelPCA\n",
        "\n",
        "    k_pca = KernelPCA(n_components=self.dimension, kernel=kernel)\n",
        "    reducted_data = k_pca.fit_transform(self.train_data)\n",
        "\n",
        "    return reducted_data\n",
        "\n",
        "  def svd(self):\n",
        "    from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "    svd = TruncatedSVD(n_components=self.dimension)\n",
        "    reducted_data = svd.fit_transform(self.train_data)\n",
        "\n",
        "    return reducted_data\n",
        "\n",
        "  def umap(self,\n",
        "           neighbors,\n",
        "           metric):\n",
        "    \n",
        "    try:\n",
        "      import umap\n",
        "    except ImportError:\n",
        "      !pip install umap-learn\n",
        "      import umap\n",
        "\n",
        "    umap = umap.UMAP(n_neighbors=neighbors, n_components=self.dimension, metric=metric)\n",
        "    embedding = umap.fit_transform(self.train_data)\n",
        "\n",
        "    return embedding\n",
        "\n",
        "\n",
        "\n",
        "class metrics:\n",
        "\n",
        "  def __init__(self,\n",
        "               data,\n",
        "               predicted_labels):\n",
        "    \n",
        "    self.data = data\n",
        "    self.predicted_labels = predicted_labels\n",
        "\n",
        "  def silhouette(self):\n",
        "    from sklearn.metrics import silhouette_score\n",
        "\n",
        "    acc_score = silhouette_score(self.data, self.predicted_labels, metric='euclidean')\n",
        "    \n",
        "    return acc_score\n",
        "\n",
        "  def f1_score(self):\n",
        "    from sklearn.metrics import f1_score\n",
        "\n",
        "    acc_score = f1_score(self.data, self.predicted_labels)\n",
        "\n",
        "    return acc_score\n",
        "\n",
        "  def roc_auc(self,\n",
        "              predicted_prob,\n",
        "              multiclass):\n",
        "    \n",
        "    from sklearn.metrics import roc_auc_score\n",
        "\n",
        "    if multiclass:\n",
        "      acc_score = roc_auc_score(self.data, predicted_prob, multiclass='ovr')\n",
        "\n",
        "    else: \n",
        "      acc_score = roc_auc_score(self.data, predicted_prob)\n",
        "\n",
        "    return acc_score\n",
        "\n",
        "  def simple_acc(self):\n",
        "    from sklearn.metrics import accuracy_score\n",
        "\n",
        "    acc_score = accuracy_score(self.data, self.predicted_labels)\n",
        "\n",
        "    return acc_score\n",
        "\n",
        "  def confusion_matrix(self):\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "\n",
        "    cm = confusion_matrix(self.data, self.predicted_labels)\n",
        "\n",
        "    return cm\n",
        "\n",
        "\n",
        "\n",
        "class algorithms:\n",
        "\n",
        "  def __init__(self,\n",
        "               train_data,\n",
        "               train_labels=None,\n",
        "               test_data=None,\n",
        "               test_labels=None):\n",
        "    \n",
        "    self.random_state = 42\n",
        "    self.train_data = train_data\n",
        "    self.train_labels = train_labels\n",
        "    self.test_data = test_data\n",
        "    self.test_labels = test_labels\n",
        "\n",
        "  def kmeans(self,\n",
        "             class_count):\n",
        "    \n",
        "    model = {}\n",
        "    \n",
        "    from sklearn.cluster import KMeans\n",
        "\n",
        "    kmeans = KMeans(n_clusters=class_count, random_state=self.random_state, n_init=20)\n",
        "    kmeans.fit(self.train_data)\n",
        "\n",
        "    acc = metrics(self.train_data, kmeans.labels_)\n",
        "    acc_score = acc.silhouette()\n",
        "\n",
        "    model['model'] = kmeans\n",
        "    model['labels'] = kmeans.labels_\n",
        "    model['acc'] = acc_score\n",
        "\n",
        "    return model\n",
        "\n",
        "  def dbscan(self,\n",
        "             eps,\n",
        "             min_samples):\n",
        "    \n",
        "    model = {}\n",
        "\n",
        "    from sklearn.cluster import DBSCAN\n",
        "\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    dbscan.fit(self.train_data)\n",
        "\n",
        "    acc = metrics(self.train_data, dbscan.labels_)\n",
        "    acc_score = acc.silhouette()\n",
        "\n",
        "    model['model'] = dbscan\n",
        "    model['labels'] = dbscan.labels_\n",
        "    model['acc'] = acc_score\n",
        "\n",
        "    return model\n",
        "\n",
        "  def svm(self,\n",
        "          kernel,\n",
        "          c):\n",
        "    \n",
        "    model = {}\n",
        "    \n",
        "    from sklearn.svm import SVC\n",
        "\n",
        "    svm = SVC(kernel=kernel, C=c, random_state=self.random_state)\n",
        "    svm.fit(self.train_data, self.train_labels)\n",
        "    predictions = svm.predict(self.test_data)\n",
        "\n",
        "    acc = metrics(self.test_labels, predictions)\n",
        "    acc_score = acc.simple_acc()\n",
        "\n",
        "    model['model'] = svm\n",
        "    model['labels'] = predictions\n",
        "    model['acc'] = acc_score\n",
        "\n",
        "    return model\n",
        "\n",
        "  def dtc(self):\n",
        "\n",
        "    model = {}\n",
        "\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "    dtc = DecisionTreeClassifier()\n",
        "    dtc.fit(self.train_data, self.train_labels)\n",
        "    predictions = dtc.predict(self.test_data)\n",
        "\n",
        "    acc = metrics(self.test_labels, predictions)\n",
        "    acc_score = acc.simple_acc()\n",
        "\n",
        "    model['model'] = dtc\n",
        "    model['labels'] = predictions\n",
        "    model['acc'] = acc_score\n",
        "\n",
        "    return model\n",
        "\n",
        "  def rf(self,\n",
        "         n_estimators,\n",
        "         max_depth,\n",
        "         max_features):\n",
        "    \n",
        "    model = {}\n",
        "    \n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "    rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features)\n",
        "    rf.fit(self.train_data, self.train_labels)\n",
        "    predictions = rf.predict(self.test_data)\n",
        "\n",
        "    acc = metrics(self.test_labels, predictions)\n",
        "    acc_score = acc.simple_acc()\n",
        "\n",
        "    model['model'] = rf\n",
        "    model['labels'] = predictions\n",
        "    model['acc'] = acc_score\n",
        "\n",
        "    return model\n",
        "\n",
        "  def mlp(self,\n",
        "          layers,\n",
        "          activation_func,\n",
        "          loss,\n",
        "          optimizer,\n",
        "          metric,\n",
        "          epochs):\n",
        "    \n",
        "    model = {}\n",
        "    \n",
        "    try:\n",
        "      from tensorflow.keras.models import Sequential\n",
        "      from tensorflow.keras.layers import Dense\n",
        "    except ImportError:\n",
        "      !pip install tensorflow\n",
        "      from tensorflow.keras.models import Sequential\n",
        "      from tensorflow.keras.layers import Dense\n",
        "\n",
        "    mlp = Sequential()\n",
        "    for i in layers:\n",
        "      mlp.add(Dense(layers[i], input_dim=self.train_data.shape[1], activation=activation_func))\n",
        "\n",
        "    mlp.add(Dense(1, activation='sigmoid'))\n",
        "    mlp.compile(loss=loss, optimizer=optimizer, metrics=metric)\n",
        "    mlp.fit(self.train_data, self.train_labels, epochs=epochs)\n",
        "\n",
        "    threshold = 0.5\n",
        "    \n",
        "    predictions = mlp.predict(self.test_data)\n",
        "    predictions = np.where(predictions > threshold, 1, 0)\n",
        "    prediction = np.squeeze(predictions)\n",
        "\n",
        "    acc = metrics(self.test_labels, predictions)\n",
        "    acc_score = acc.simple_acc()\n",
        "\n",
        "    model['model'] = mlp\n",
        "    model['labels'] = predictions\n",
        "    model['acc'] = acc_score\n",
        "\n",
        "    return model\n",
        "\n",
        "  def pca(self,\n",
        "          dimension):\n",
        "    \n",
        "    pca = reduction(self.train_data, dimension)\n",
        "\n",
        "    return pca.pca()\n",
        "\n",
        "  def kernel_pca(self,\n",
        "                 dimension,\n",
        "                 kernel):\n",
        "    \n",
        "    k_pca = reduction(self.train_data, dimension)\n",
        "\n",
        "    return k_pca.kernel_pca(kernel)\n",
        "\n",
        "  def svd(self,\n",
        "          dimension):\n",
        "    \n",
        "    svd = reduction(self.train_data, dimension)\n",
        "\n",
        "    return svd.svd()\n",
        "\n",
        "  def umap(self,\n",
        "           neighbors,\n",
        "           dimension,\n",
        "           metric):\n",
        "    umap = reduction(self.train_data, dimension)\n",
        "    \n",
        "    return umap.umap(neighbors, metric)"
      ],
      "metadata": {
        "id": "PqVO1LCrcg6l"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dims = [3, 5, 10]\n",
        "\n",
        "PCA_data = []\n",
        "SVD_data = []\n",
        "\n",
        "for i in dims:\n",
        "  reducer = algorithms(scaled_df)\n",
        "  \n",
        "  reduced = reducer.kernel_pca(i, 'rbf')\n",
        "  PCA_data.append(reduced)\n",
        "\n",
        "  reduced = reducer.svd(i)\n",
        "  SVD_data.append(reduced)\n",
        "\n",
        "  divided_PCA = []\n",
        "  divided_SVD = []\n",
        "\n",
        "  for i in PCA_data:\n",
        "    divided_PCA.append(i)\n",
        "\n",
        "  for i in SVD_data:\n",
        "    divided_SVD.append(i)"
      ],
      "metadata": {
        "id": "66_nrHCociNW"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "for i, val in enumerate(divided_PCA):\n",
        "\n",
        "  kmeans = algorithms(val)\n",
        "  labels = kmeans.kmeans(class_count)['labels']\n",
        "\n",
        "  acc = metrics(val, labels)\n",
        "  score = acc.silhouette()\n",
        "\n",
        "  plt.scatter(val[:, 0], val[:, 1], c=labels, s=50, cmap='plasma')\n",
        "  plt.title(f'PCA {str(i)} Score : {score}')\n",
        "  plt.show()\n",
        "\n",
        "  fig = plt.figure(figsize=(8, 8)) \n",
        "  ax = plt.axes(projection='3d') \n",
        "  ax.scatter(val[:, 0], val[:, 1], val[:, 2], c=labels, cmap='plasma') \n",
        "  ax.set_title('PCA ' + str(i)) \n",
        "  plt.show()\n",
        "\n",
        "for i, val in enumerate(divided_SVD):\n",
        "\n",
        "  kmeans = algorithms(val)\n",
        "  labels = kmeans.kmeans(class_count)['labels']\n",
        "\n",
        "  acc = metrics(val, labels)\n",
        "  score = acc.silhouette()\n",
        "\n",
        "  plt.scatter(val[:, 0], val[:, 1], c=labels)\n",
        "  plt.title(f'SVD {str(i)} Score : {score}')\n",
        "  plt.show()\n",
        "  \n",
        "  fig = plt.figure(figsize=(8, 8)) \n",
        "  ax = plt.axes(projection='3d') \n",
        "  ax.scatter(val[:, 0], val[:, 1], val[:, 2], c=labels, cmap='viridis') \n",
        "  ax.set_title('SVD ' + str(i)) \n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "8HQE2cR7FGtG"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "reducer = algorithms(scaled_df)\n",
        "embedding = reducer.umap(2, 2, 'euclidean')\n",
        "\n",
        "kmeans = algorithms(embedding)\n",
        "labels = kmeans.kmeans(class_count)['labels']\n",
        "\n",
        "acc = metrics(embedding, labels)\n",
        "score = acc.silhouette()\n",
        "print(score)\n",
        "\n",
        "plt.scatter(embedding[:, 0], embedding[:, 1], c=labels, cmap='plasma')\n",
        "plt.title('UMAP ' + str(i))\n",
        "plt.show()\n",
        "\n",
        "# fig = plt.figure(figsize=(6, 6))\n",
        "# ax = plt.axes(projection='3d')\n",
        "# ax.scatter(embedding[:, 0], embedding[:, 1], embedding[:, 2], c=labels, cmap='plasma')\n",
        "# ax.set_title('UMAP')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "UHOSr9hNbFb3"
      },
      "execution_count": 101,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}